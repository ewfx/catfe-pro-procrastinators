{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "#%pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "context_file_path = \"./models/context_files/context.txt\"\n",
    "functionality_map_path = \"./models/context_files/functionality_map.json\"\n",
    "test_output_dir = \"./models/output/tests\"\n",
    "contextMapped_path = \"./models/context_files/contextMapped.txt\"  # Define the path for contextMapped\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(\"./models/output\", exist_ok=True)\n",
    "os.makedirs(\"./models/context_files\", exist_ok=True)\n",
    "os.makedirs(test_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "controllers = os.listdir(\"../code/src/main/java/com/bank/controllers\")\n",
    "entities = os.listdir(\"../code/src/main/java/com/bank/models\")\n",
    "repositories = os.listdir(\"../code/src/main/java/com/bank/repositories\")\n",
    "services = os.listdir(\"../code/src/main/java/com/bank/services\")\n",
    "app_path = \"../code/src/main/java/com/bank/BankingApiApplication.java\"\n",
    "context_path = \"../code/src/main/resources/application.properties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = \"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\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file = f.read()\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Read me this file of my java springboot application in the variable \" + file + \" and describe the functionality and context of the code written in the file\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        stream=False,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_context_file(context_file_path):\n",
    "    with open(context_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Initialize variables to track the current section and file\n",
    "    sections = [\"CONTROLLER CONTEXT\", \"ENTITY/MODEL CONTEXT\", \"REPOSITORY CONTEXT\", \"SERVICE CONTEXT\", \n",
    "                \"APPLICATION CONTEXT\", \"APPLICATION PROPERTIES CONTEXT\", \"APPLICATION PROPERTIES CONTEXT\"]\n",
    "    \n",
    "    file_contexts = {}\n",
    "    current_section = None\n",
    "    current_file = None\n",
    "    current_context = []\n",
    "    \n",
    "    # Process the file line by line\n",
    "    lines = content.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        # Check if this line starts a new section\n",
    "        if any(line.strip() == section for section in sections):\n",
    "            current_section = line.strip()\n",
    "            continue\n",
    "            \n",
    "        # Check if this line starts a new file within a section\n",
    "        for prefix in [\"CONTROLLER:\", \"ENTITY:\", \"REPOSITORY:\", \"SERVICE:\"]:\n",
    "            if line.strip().startswith(prefix):\n",
    "                # Save previous file context if exists\n",
    "                if current_file and current_context:\n",
    "                    file_contexts[current_file] = \"\\n\".join(current_context)\n",
    "                    current_context = []\n",
    "                \n",
    "                # Start new file\n",
    "                current_file = line.strip()[len(prefix):].strip()\n",
    "                break\n",
    "        else:\n",
    "            # If not a section or file header, add to current context\n",
    "            if current_file:\n",
    "                current_context.append(line)\n",
    "    \n",
    "    # Save the last file context\n",
    "    if current_file and current_context:\n",
    "        file_contexts[current_file] = \"\\n\".join(current_context)\n",
    "    \n",
    "    return file_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize context output content\n",
    "context_output_content = \"CONTROLLER CONTEXT\\n\\n\"\n",
    "for controller in controllers:\n",
    "    context_output_content += \"CONTROLLER: \" + controller + \"\\n\\n\"\n",
    "    context_output_content += get_context(\"../code/src/main/java/com/bank/controllers/\" + controller) + \"\\n\\n\"\n",
    "\n",
    "context_output_content += \"ENTITY CONTEXT\\n\\n\"\n",
    "for entity in entities:\n",
    "    context_output_content += \"ENTITY: \" + entity + \"\\n\\n\"\n",
    "    context_output_content += get_context(\"../code/src/main/java/com/bank/models/\" + entity) + \"\\n\\n\"\n",
    "\n",
    "context_output_content += \"REPOSITORY CONTEXT\\n\\n\"\n",
    "for repository in repositories:\n",
    "    context_output_content += \"REPOSITORY: \" + repository + \"\\n\\n\"\n",
    "    context_output_content += get_context(\"../code/src/main/java/com/bank/repositories/\" + repository) + \"\\n\\n\"\n",
    "\n",
    "context_output_content += \"SERVICE CONTEXT\\n\\n\"\n",
    "for service in services:\n",
    "    context_output_content += \"SERVICE: \" + service + \"\\n\\n\"\n",
    "    context_output_content += get_context(\"../code/src/main/java/com/bank/services/\" + service) + \"\\n\\n\"\n",
    "\n",
    "context_output_content += \"APPLICATION CONTEXT\\n\\n\"\n",
    "context_output_content += get_context(app_path) + \"\\n\\n\"\n",
    "\n",
    "context_output_content += \"APPLICATION PROPERTIES CONTEXT\\n\\n\"\n",
    "context_output_content += get_context(context_path) + \"\\n\\n\"\n",
    "\n",
    "# Write to contextMapped file\n",
    "with open(contextMapped_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(context_output_content)\n",
    "\n",
    "# Store raw context in context_file_path for use with create_functionality_map\n",
    "with open(context_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(context_output_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functionality_map(file_contexts):\n",
    "    \"\"\"\n",
    "    Analyze file contexts to create a functionality map\n",
    "    \"\"\"\n",
    "    # Create context string for the AI prompt\n",
    "    context_summary = \"\"\n",
    "    for file, context in file_contexts.items():\n",
    "        # Limit context to avoid token limits\n",
    "        summary = context[:1000] + \"...\" if len(context) > 1000 else context\n",
    "        context_summary += f\"File: {file}\\nContext:\\n{summary}\\n\\n\"\n",
    "    \n",
    "    # Prompt for the AI to create a functionality map\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following file contexts from a Java Spring Boot application and identify all business functionalities.\n",
    "    For each functionality, list the files that implement it.\n",
    "    \n",
    "    {context_summary}\n",
    "    \n",
    "    Create a JSON object with the following structure:\n",
    "    {{\n",
    "        \"functionalities\": [\n",
    "            {{\n",
    "                \"name\": \"functionality_name\",\n",
    "                \"description\": \"description of what this functionality does\",\n",
    "                \"files\": [\"file_path1\", \"file_path2\"],\n",
    "                \"primary_files\": [\"main_file_path\"],\n",
    "                \"supporting_files\": [\"supporting_file_path1\", \"supporting_file_path2\"],\n",
    "                \"category\": \"category (e.g., 'core', 'security', 'data')\",\n",
    "                \"complexity\": \"high/medium/low\",\n",
    "                \"test_priority\": \"high/medium/low\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Group files by actual business functionality (like \"User Authentication\", \"Journal Entry Management\", etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert Java analyzer that identifies business functionalities from code context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        stream=False,\n",
    "    )\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Try to find JSON in the response\n",
    "    json_start = response_text.find(\"{\")\n",
    "    json_end = response_text.rfind(\"}\")\n",
    "    \n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = response_text[json_start:json_end+1]\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to clean up the JSON string\n",
    "            # This handles some common issues like unescaped newlines\n",
    "            cleaned_json = re.sub(r'(?<!\\\\)\"(?=(,|\\s*}|\\s*]|\\n))', '\\\\\"', json_str)\n",
    "            try:\n",
    "                return json.loads(cleaned_json)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error parsing JSON response. Using fallback parsing.\")\n",
    "                # Fallback to a more robust but simplistic parsing approach\n",
    "                return {\"functionalities\": [{\"name\": \"Generic Functionality\", \"files\": list(file_contexts.keys())}]}\n",
    "    else:\n",
    "        print(\"Could not find JSON in response. Using empty functionality map.\")\n",
    "        return {\"functionalities\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_cases(functionality_map):\n",
    "    \"\"\"\n",
    "    Generate test cases for each functionality in the map\n",
    "    \"\"\"\n",
    "    functionalities = functionality_map.get(\"functionalities\", [])\n",
    "    \n",
    "    for functionality in functionalities:\n",
    "        functionality_name = functionality[\"name\"]\n",
    "        sanitized_name = functionality_name.replace(\" \", \"_\").lower()\n",
    "        print(f\"Generating tests for {functionality_name}...\")\n",
    "        \n",
    "        # Create file names\n",
    "        feature_file_name = f\"{sanitized_name}.feature\"\n",
    "        step_def_file_name = f\"{sanitized_name}_steps.java\"\n",
    "        \n",
    "        # Get the file contexts for this functionality\n",
    "        file_list = functionality.get(\"files\", [])\n",
    "        contexts = {}\n",
    "        for file in file_list:\n",
    "            if file in file_contexts:\n",
    "                # Limit context size to avoid token limits\n",
    "                context = file_contexts[file]\n",
    "                contexts[file] = context[:2000] + \"...\" if len(context) > 2000 else context\n",
    "        \n",
    "        # Create a summary of file contexts\n",
    "        context_summary = \"\\n\\n\".join([f\"File: {file}\\nContext:\\n{context}\" for file, context in contexts.items()])\n",
    "        \n",
    "        # Prompt for the AI to generate test cases\n",
    "        prompt = f\"\"\"\n",
    "        Generate comprehensive test cases for the '{functionality_name}' functionality in a Java Spring Boot application, which should have full implementation instead of comments.\n",
    "\n",
    "        Functionality details:\n",
    "        {json.dumps(functionality, indent=2)}\n",
    "        \n",
    "        Context of relevant files:\n",
    "        {context_summary}\n",
    "        \n",
    "        Create two files:\n",
    "        \n",
    "        1. A Cucumber feature file (.feature) with:\n",
    "           - Feature description\n",
    "           - Background (if needed)\n",
    "           - Multiple scenarios covering happy paths, error paths, and edge cases\n",
    "           - Use Gherkin syntax (Given/When/Then)\n",
    "        \n",
    "        2. Java step definitions with:\n",
    "           - All the required @Given, @When, @Then annotations\n",
    "           - Implementation for each step\n",
    "           - Appropriate assertions\n",
    "           - Any required mocks or test data setup\n",
    "        \n",
    "        Make sure the steps in the feature file match exactly with those in the step definitions.\n",
    "        Include comprehensive test coverage for this functionality.\n",
    "        \n",
    "        Respond with:\n",
    "        [FEATURE FILE START]\n",
    "        Feature: ...\n",
    "        ...\n",
    "        [FEATURE FILE END]\n",
    "        \n",
    "        [STEP DEFINITIONS START]\n",
    "        package ...\n",
    "        ...\n",
    "        [STEP DEFINITIONS END]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert test automation engineer specializing in BDD testing with Cucumber for Java Spring Boot applications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "            stream=False,\n",
    "        )\n",
    "        \n",
    "        # Extract feature file and step definitions from response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse feature file\n",
    "        feature_start = response_text.find(\"[FEATURE FILE START]\")\n",
    "        feature_end = response_text.find(\"[FEATURE FILE END]\")\n",
    "        \n",
    "        if feature_start >= 0 and feature_end > feature_start:\n",
    "            feature_content = response_text[feature_start + len(\"[FEATURE FILE START]\"):feature_end].strip()\n",
    "            \n",
    "            # Save feature file\n",
    "            feature_file_path = os.path.join(test_output_dir, feature_file_name)\n",
    "            with open(feature_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(feature_content)\n",
    "            print(f\"Created feature file: {feature_file_path}\")\n",
    "        \n",
    "        # Parse step definitions\n",
    "        step_def_start = response_text.find(\"[STEP DEFINITIONS START]\")\n",
    "        step_def_end = response_text.find(\"[STEP DEFINITIONS END]\")\n",
    "        \n",
    "        if step_def_start >= 0 and step_def_end > step_def_start:\n",
    "            step_def_content = response_text[step_def_start + len(\"[STEP DEFINITIONS START]\"):step_def_end].strip()\n",
    "            \n",
    "            # Save step definitions file\n",
    "            step_def_file_path = os.path.join(test_output_dir, step_def_file_name)\n",
    "            with open(step_def_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(step_def_content)\n",
    "            print(f\"Created step definitions file: {step_def_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_summary(functionality_map):\n",
    "    \"\"\"\n",
    "    Generate a summary of all tests created\n",
    "    \"\"\"\n",
    "    functionalities = functionality_map.get(\"functionalities\", [])\n",
    "    \n",
    "    summary = \"# Test Suite Summary\\n\\n\"\n",
    "    summary += \"| Functionality | Test Priority | Feature File | Step Definitions |\\n\"\n",
    "    summary += \"|--------------|--------------|-------------|------------------|\\n\"\n",
    "    \n",
    "    for functionality in functionalities:\n",
    "        name = functionality[\"name\"]\n",
    "        priority = functionality.get(\"test_priority\", \"medium\")\n",
    "        sanitized_name = name.replace(\" \", \"_\").lower()\n",
    "        feature_file = f\"{sanitized_name}.feature\"\n",
    "        step_def_file = f\"{sanitized_name}_steps.java\"\n",
    "        \n",
    "        summary += f\"| {name} | {priority} | [{feature_file}](tests/{feature_file}) | [{step_def_file}](tests/{step_def_file}) |\\n\"\n",
    "    \n",
    "    # Save summary\n",
    "    test_summary_path = os.path.join(\"./models/output\", \"test_summary.md\")\n",
    "    with open(test_summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "    print(f\"Created test summary: {test_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test generation process...\n",
      "Parsing context file...\n",
      "Found contexts for 8 files\n",
      "Creating functionality map...\n",
      "Functionality map saved to ./models/context_files/functionality_map.json\n",
      "Generating test cases...\n",
      "Generating tests for Account Management...\n",
      "Generating tests for Transaction Management...\n",
      "Generating tests for Fraud Detection...\n",
      "Generating test summary...\n",
      "Created test summary: ./models/output\\test_summary.md\n",
      "Test generation completed!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting test generation process...\")\n",
    "    \n",
    "    # Step 1: Parse context file\n",
    "    print(\"Parsing context file...\")\n",
    "    file_contexts = parse_context_file(context_file_path)\n",
    "    print(f\"Found contexts for {len(file_contexts)} files\")\n",
    "    \n",
    "    # Step 2: Create functionality map\n",
    "    print(\"Creating functionality map...\")\n",
    "    functionality_map = create_functionality_map(file_contexts)\n",
    "    \n",
    "    # Save functionality map\n",
    "    with open(functionality_map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(functionality_map, f, indent=2)\n",
    "    print(f\"Functionality map saved to {functionality_map_path}\")\n",
    "    \n",
    "    # Step 3: Generate test cases\n",
    "    print(\"Generating test cases...\")\n",
    "    generate_test_cases(functionality_map)\n",
    "    \n",
    "    # Step 4: Generate test summary\n",
    "    print(\"Generating test summary...\")\n",
    "    generate_test_summary(functionality_map)\n",
    "    \n",
    "    print(\"Test generation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
