{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (1.68.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.1.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "from openai import OpenAI\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "code_dir = \"../code/\"\n",
    "context_file_path = \"./context_files/context.txt\"\n",
    "functionality_map_path = \"./output/functionality_map.json\"\n",
    "test_output_dir = \"./output/tests\"\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(test_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = \"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\",\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file = f.read()\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Read me this file of my java springboot application in the variable \" + file + \" and describe the functionality and context of the code written in the file\"\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        stream=False,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_context(context_file_path, test_output_dir):\n",
    "    # Loop through all files in the test directory\n",
    "    test_context = \"TEST CONTEXT\\n\\n\"\n",
    "    for file in os.listdir(test_output_dir):\n",
    "        # Get the test context\n",
    "        test_context += \"TEST FILE: \" + file + \"\\n\\n\"\n",
    "        with open(test_output_dir + '/' + file, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_content = f.read()\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Read me this file of that contains functional test cases in the form of gherkin/step_definitions \" + file_content + \" and describe the context of the tests/methods written in the file\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "            stream=False,\n",
    "            temperature=0,\n",
    "        )\n",
    "        test_context += chat_completion.choices[0].message.content\n",
    "        # Write the context to the context file\n",
    "        with open(context_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(test_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_context_file(context_file_path):\n",
    "    with open(context_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Initialize variables to track the current section and file\n",
    "    sections = [\"CONTROLLER CONTEXT\", \"ENTITY CONTEXT\", \"REPOSITORY CONTEXT\", \"SERVICE CONTEXT\", \n",
    "                \"APPLICATION CONTEXT\", \"APPLICATION PROPERTIES CONTEXT\"]\n",
    "    \n",
    "    file_contexts = {}\n",
    "    current_section = None\n",
    "    current_file = None\n",
    "    current_context = []\n",
    "    \n",
    "    # Process the file line by line\n",
    "    lines = content.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        # Check if this line starts a new section\n",
    "        if any(line.strip() == section for section in sections):\n",
    "            current_section = line.strip()\n",
    "            continue\n",
    "            \n",
    "        # Check if this line starts a new file within a section\n",
    "        for prefix in [\"CONTROLLER:\", \"ENTITY:\", \"REPOSITORY:\", \"SERVICE:\"]:\n",
    "            if line.strip().startswith(prefix):\n",
    "                # Save previous file context if exists\n",
    "                if current_file and current_context:\n",
    "                    file_contexts[current_file] = \"\\n\".join(current_context)\n",
    "                    current_context = []\n",
    "                \n",
    "                # Start new file\n",
    "                current_file = line.strip()[len(prefix):].strip()\n",
    "                break\n",
    "        else:\n",
    "            # If not a section or file header, add to current context\n",
    "            if current_file:\n",
    "                current_context.append(line)\n",
    "    \n",
    "    # Save the last file context\n",
    "    if current_file and current_context:\n",
    "        file_contexts[current_file] = \"\\n\".join(current_context)\n",
    "    \n",
    "    return file_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper call for you to determine your directory if needed\n",
    "# print(os.getcwd())\n",
    "\n",
    "# controllers = os.listdir(\"../code/src/main/java/net/engineeringdigest/journalApp/controller\")\n",
    "# entities = os.listdir(\"../code/src/main/java/net/engineeringdigest/journalApp/entity\")\n",
    "# repositories = os.listdir(\"../code/src/main/java/net/engineeringdigest/journalApp/repository\")\n",
    "# services = os.listdir(\"../code/src/main/java/net/engineeringdigest/journalApp/service\")\n",
    "# app_path = \"../code/src/main/java/net/engineeringdigest/journalApp/JournalApplication.java\"\n",
    "# context_path = \"../code/src/main/resources/application.properties\"\n",
    "controllers = os.listdir(\"../code/src/main/java/com/bank/controllers\")\n",
    "entities = os.listdir(\"../code/src/main/java/com/bank/models\")\n",
    "repositories = os.listdir(\"../code/src/main/java/com/bank/repositories\")\n",
    "services = os.listdir(\"../code/src/main/java/com/bank/services\")\n",
    "app_path = \"../code/src/main/java/com/bank/BankingApiApplication.java\"\n",
    "context_path = \"../code/src/main/resources/application.properties\"\n",
    "\n",
    "context_output_path = \"./context_files/context.txt\"\n",
    "diff_output_file_path = \"changed_and_untracked_files.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_context():\n",
    "    context_output = \"CONTROLLER CONTEXT\\n\\n\"\n",
    "    for controller in controllers:\n",
    "        context_output += \"CONTROLLER: \" + controller + \"\\n\\n\"\n",
    "        context_output += get_context(\"../code/src/main/java/com/bank/controllers/\" + controller) + \"\\n\\n\"\n",
    "\n",
    "    context_output += \"ENTITY CONTEXT\\n\\n\"\n",
    "    for entity in entities:\n",
    "        context_output += \"ENTITY: \" + entity + \"\\n\\n\"\n",
    "        context_output += get_context(\"../code/src/main/java/com/bank/models/\" + entity) + \"\\n\\n\"\n",
    "\n",
    "    context_output += \"REPOSITORY CONTEXT\\n\\n\"\n",
    "    for repository in repositories:\n",
    "        context_output += \"REPOSITORY: \" + repository + \"\\n\\n\"\n",
    "        context_output += get_context(\"../code/src/main/java/com/bank/repositories/\" + repository) + \"\\n\\n\"\n",
    "\n",
    "    context_output += \"SERVICE CONTEXT\\n\\n\"\n",
    "    for service in services:\n",
    "        context_output += \"SERVICE: \" + service + \"\\n\\n\"\n",
    "        context_output += get_context(\"../code/src/main/java/com/bank/services/\" + service) + \"\\n\\n\"\n",
    "\n",
    "    context_output += \"APPLICATION CONTEXT\\n\\n\"\n",
    "    context_output += get_context(app_path) + \"\\n\\n\"\n",
    "\n",
    "    context_output += \"APPLICATION PROPERTIES CONTEXT\\n\\n\"\n",
    "    context_output += get_context(context_path) + \"\\n\\n\"\n",
    "\n",
    "    open(context_output_path, \"w\", encoding=\"utf-8\").write(context_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functionality_map(file_contexts):\n",
    "    \"\"\"\n",
    "    Analyze file contexts to create a functionality map\n",
    "    \"\"\"\n",
    "    # Create context string for the AI prompt\n",
    "    context_summary = \"\"\n",
    "    for file, context in file_contexts.items():\n",
    "        # Limit context to avoid token limits\n",
    "        summary = context[:1000] + \"...\" if len(context) > 1000 else context\n",
    "        context_summary += f\"File: {file}\\nContext:\\n{summary}\\n\\n\"\n",
    "    \n",
    "    # Prompt for the AI to create a functionality map\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following file contexts from a Java Spring Boot application and identify all business functionalities.\n",
    "    For each functionality, list the files that implement it.\n",
    "    \n",
    "    {context_summary}\n",
    "    \n",
    "    Create a JSON object with the following structure:\n",
    "    {{\n",
    "        \"functionalities\": [\n",
    "            {{\n",
    "                \"name\": \"functionality_name\",\n",
    "                \"description\": \"description of what this functionality does\",\n",
    "                \"files\": [\"file_path1\", \"file_path2\"],\n",
    "                \"primary_files\": [\"main_file_path\"],\n",
    "                \"supporting_files\": [\"supporting_file_path1\", \"supporting_file_path2\"],\n",
    "                \"category\": \"category (e.g., 'core', 'security', 'data')\",\n",
    "                \"complexity\": \"high/medium/low\",\n",
    "                \"test_priority\": \"high/medium/low\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Group files by actual business functionality (like \"User Authentication\", \"Journal Entry Management\", etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert Java analyzer that identifies business functionalities from code context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        stream=False,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Try to find JSON in the response\n",
    "    json_start = response_text.find(\"{\")\n",
    "    json_end = response_text.rfind(\"}\")\n",
    "    \n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = response_text[json_start:json_end+1]\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to clean up the JSON string\n",
    "            # This handles some common issues like unescaped newlines\n",
    "            cleaned_json = re.sub(r'(?<!\\\\)\"(?=(,|\\s*}|\\s*]|\\n))', '\\\\\"', json_str)\n",
    "            try:\n",
    "                return json.loads(cleaned_json)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error parsing JSON response. Using fallback parsing.\")\n",
    "                # Fallback to a more robust but simplistic parsing approach\n",
    "                return {\"functionalities\": [{\"name\": \"Generic Functionality\", \"files\": list(file_contexts.keys())}]}\n",
    "    else:\n",
    "        print(\"Could not find JSON in response. Using empty functionality map.\")\n",
    "        return {\"functionalities\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functionality_test_map(file_contexts):\n",
    "    \"\"\"\n",
    "    Analyze file contexts to create a functionality test case map\n",
    "    \"\"\"\n",
    "    # Create context string for the AI prompt\n",
    "    context_summary = \"\"\n",
    "    for file, context in file_contexts.items():\n",
    "        # Limit context to avoid token limits\n",
    "        summary = context[:1000] + \"...\" if len(context) > 1000 else context\n",
    "        context_summary += f\"File: {file}\\nContext:\\n{summary}\\n\\n\"\n",
    "    \n",
    "    # Prompt for the AI to create a functionality map\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following file contexts from a Java Spring Boot application and identify all business functionalities along with all the test cases associated with each functionality.\n",
    "    For each functionality, list the files that implement it.\n",
    "    \n",
    "    {context_summary}\n",
    "    \n",
    "    Create a JSON object with the following structure:\n",
    "    {{\n",
    "        \"functionalities\": [\n",
    "            {{\n",
    "                \"name\": \"functionality_name\",\n",
    "                \"description\": \"description of what this functionality does\",\n",
    "                \"files\": [\"file_path1\", \"file_path2\"],\n",
    "                \"primary_files\": [\"main_file_path\"],\n",
    "                \"supporting_files\": [\"supporting_file_path1\", \"supporting_file_path2\"],\n",
    "                \"test_cases\": [\"feature1\", \"step_definition1\"],\n",
    "                \"test_description\": \"description of the test cases associated with the functionality\",\n",
    "                \"test_details\": \"details of the test cases as written in step_definitions associated with the functionality\",\n",
    "                \"category\": \"category (e.g., 'core', 'security', 'data')\",\n",
    "                \"complexity\": \"high/medium/low\",\n",
    "                \"test_priority\": \"high/medium/low\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Group files by actual business functionality (like \"User Authentication\", \"Journal Entry Management\", etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are an expert Java analyzer that identifies business functionalities from code and test cases context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "        stream=False,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    response_text = response.choices[0].message.content\n",
    "    \n",
    "    # Try to find JSON in the response\n",
    "    json_start = response_text.find(\"{\")\n",
    "    json_end = response_text.rfind(\"}\")\n",
    "    \n",
    "    if json_start >= 0 and json_end > json_start:\n",
    "        json_str = response_text[json_start:json_end+1]\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to clean up the JSON string\n",
    "            # This handles some common issues like unescaped newlines\n",
    "            cleaned_json = re.sub(r'(?<!\\\\)\"(?=(,|\\s*}|\\s*]|\\n))', '\\\\\"', json_str)\n",
    "            try:\n",
    "                return json.loads(cleaned_json)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error parsing JSON response. Using fallback parsing.\")\n",
    "                # Fallback to a more robust but simplistic parsing approach\n",
    "                return {\"functionalities\": [{\"name\": \"Generic Functionality\", \"files\": list(file_contexts.keys())}]}\n",
    "    else:\n",
    "        print(\"Could not find JSON in response. Using empty functionality map.\")\n",
    "        return {\"functionalities\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_cases(functionality_map):\n",
    "    \"\"\"\n",
    "    Generate test cases for each functionality in the map\n",
    "    \"\"\"\n",
    "    functionalities = functionality_map.get(\"functionalities\", [])\n",
    "    \n",
    "    for functionality in functionalities:\n",
    "        functionality_name = functionality[\"name\"]\n",
    "        sanitized_name = functionality_name.replace(\" \", \"_\").lower()\n",
    "        print(f\"Generating tests for {functionality_name}...\")\n",
    "        \n",
    "        # Create file names\n",
    "        feature_file_name = f\"{sanitized_name}.feature\"\n",
    "        step_def_file_name = f\"{sanitized_name}_steps.java\"\n",
    "        \n",
    "        # Get the file contexts for this functionality\n",
    "        file_list = functionality.get(\"files\", [])\n",
    "        contexts = {}\n",
    "        for file in file_list:\n",
    "            if file in file_contexts:\n",
    "                # Limit context size to avoid token limits\n",
    "                context = file_contexts[file]\n",
    "                contexts[file] = context[:2000] + \"...\" if len(context) > 2000 else context\n",
    "        \n",
    "        # Create a summary of file contexts\n",
    "        context_summary = \"\\n\\n\".join([f\"File: {file}\\nContext:\\n{context}\" for file, context in contexts.items()])\n",
    "        \n",
    "        # Prompt for the AI to generate test cases\n",
    "        prompt = f\"\"\"\n",
    "        Generate comprehensive test cases for the '{functionality_name}' functionality in a Java Spring Boot application, which should have full implementation instead of comments.\n",
    "\n",
    "        Functionality details:\n",
    "        {json.dumps(functionality, indent=2)}\n",
    "        \n",
    "        Context of relevant files:\n",
    "        {context_summary}\n",
    "        \n",
    "        Create two files:\n",
    "        \n",
    "        1. A Cucumber feature file (.feature) with:\n",
    "           - Feature description\n",
    "           - Background (if needed)\n",
    "           - Multiple scenarios covering happy paths, error paths, and edge cases\n",
    "           - Use Gherkin syntax (Given/When/Then)\n",
    "        \n",
    "        2. Java step definitions with:\n",
    "           - All the required @Given, @When, @Then annotations\n",
    "           - Implementation for each step\n",
    "           - Appropriate assertions\n",
    "           - Any required mocks or test data setup\n",
    "        \n",
    "        Make sure the steps in the feature file match exactly with those in the step definitions.\n",
    "        Include comprehensive test coverage for this functionality.\n",
    "        \n",
    "        Respond with:\n",
    "        [FEATURE FILE START]\n",
    "        Feature: ...\n",
    "        ...\n",
    "        [FEATURE FILE END]\n",
    "        \n",
    "        [STEP DEFINITIONS START]\n",
    "        package ...\n",
    "        ...\n",
    "        [STEP DEFINITIONS END]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert test automation engineer specializing in BDD testing with Cucumber for Java Spring Boot applications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "            stream=False,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract feature file and step definitions from response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse feature file\n",
    "        feature_start = response_text.find(\"[FEATURE FILE START]\")\n",
    "        feature_end = response_text.find(\"[FEATURE FILE END]\")\n",
    "        \n",
    "        if feature_start >= 0 and feature_end > feature_start:\n",
    "            feature_content = response_text[feature_start + len(\"[FEATURE FILE START]\"):feature_end].strip()\n",
    "            \n",
    "            # Save feature file\n",
    "            feature_file_path = os.path.join(test_output_dir, feature_file_name)\n",
    "            with open(feature_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(feature_content)\n",
    "            print(f\"Created feature file: {feature_file_path}\")\n",
    "        \n",
    "        # Parse step definitions\n",
    "        step_def_start = response_text.find(\"[STEP DEFINITIONS START]\")\n",
    "        step_def_end = response_text.find(\"[STEP DEFINITIONS END]\")\n",
    "        \n",
    "        if step_def_start >= 0 and step_def_end > step_def_start:\n",
    "            step_def_content = response_text[step_def_start + len(\"[STEP DEFINITIONS START]\"):step_def_end].strip()\n",
    "            \n",
    "            # Save step definitions file\n",
    "            step_def_file_path = os.path.join(test_output_dir, step_def_file_name)\n",
    "            with open(step_def_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(step_def_content)\n",
    "            print(f\"Created step definitions file: {step_def_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_test_cases(functionality_map, diff_output):\n",
    "    \"\"\"\n",
    "    Update test cases for each functionality that is updated in the map\n",
    "    \"\"\"\n",
    "    functionalities = functionality_map.get(\"functionalities\", [])\n",
    "    \n",
    "    for functionality in functionalities:\n",
    "        functionality_name = functionality[\"name\"]\n",
    "        sanitized_name = functionality_name.replace(\" \", \"_\").lower()\n",
    "        print(f\"Updating tests for {functionality_name}...\")\n",
    "        \n",
    "        # Create file names\n",
    "        feature_file_name = f\"{sanitized_name}.feature\"\n",
    "        step_def_file_name = f\"{sanitized_name}_steps.java\"\n",
    "        \n",
    "        # Get the file contexts for this functionality\n",
    "        file_list = functionality.get(\"files\", [])\n",
    "        contexts = {}\n",
    "        for file in file_list:\n",
    "            if file in file_contexts:\n",
    "                # Limit context size to avoid token limits\n",
    "                context = file_contexts[file]\n",
    "                contexts[file] = context[:2000] + \"...\" if len(context) > 2000 else context\n",
    "        \n",
    "        # Create a summary of file contexts\n",
    "        context_summary = \"\\n\\n\".join([f\"File: {file}\\nContext:\\n{context}\" for file, context in contexts.items()])\n",
    "        \n",
    "        # Prompt for the AI to generate test cases\n",
    "        prompt = f\"\"\"\n",
    "        Generate comprehensive test cases for the '{functionality_name}' functionality in a Java Spring Boot application, which should have full implementation instead of comments.\n",
    "        But make sure to only update the test cases for the functionalities that have been changed in the CODE CONTEXT or is newly added as mentioned in the diff provided: '{diff_output}'.\n",
    "        \n",
    "        Functionality details:\n",
    "        {json.dumps(functionality, indent=2)}\n",
    "        \n",
    "        Context of relevant files:\n",
    "        {context_summary}\n",
    "        \n",
    "        Create two files:\n",
    "        \n",
    "        1. A Cucumber feature file (.feature) with:\n",
    "           - Feature description\n",
    "           - Background (if needed)\n",
    "           - Multiple scenarios covering happy paths, error paths, and edge cases\n",
    "           - Use Gherkin syntax (Given/When/Then)\n",
    "        \n",
    "        2. Java step definitions with:\n",
    "           - All the required @Given, @When, @Then annotations\n",
    "           - Implementation for each step\n",
    "           - Appropriate assertions\n",
    "           - Any required mocks or test data setup\n",
    "        \n",
    "        Make sure the steps in the feature file match exactly with those in the step definitions.\n",
    "        Include comprehensive test coverage for this functionality.\n",
    "        \n",
    "        Respond with:\n",
    "        [FEATURE FILE START]\n",
    "        Feature: ...\n",
    "        ...\n",
    "        [FEATURE FILE END]\n",
    "        \n",
    "        [STEP DEFINITIONS START]\n",
    "        package ...\n",
    "        ...\n",
    "        [STEP DEFINITIONS END]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are an expert test automation engineer specializing in BDD testing with Cucumber for Java Spring Boot applications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "            stream=False,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extract feature file and step definitions from response\n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse feature file\n",
    "        feature_start = response_text.find(\"[FEATURE FILE START]\")\n",
    "        feature_end = response_text.find(\"[FEATURE FILE END]\")\n",
    "        \n",
    "        if feature_start >= 0 and feature_end > feature_start:\n",
    "            feature_content = response_text[feature_start + len(\"[FEATURE FILE START]\"):feature_end].strip()\n",
    "            \n",
    "            # Save feature file\n",
    "            feature_file_path = os.path.join(test_output_dir, feature_file_name)\n",
    "            with open(feature_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(feature_content)\n",
    "            print(f\"Created feature file: {feature_file_path}\")\n",
    "        \n",
    "        # Parse step definitions\n",
    "        step_def_start = response_text.find(\"[STEP DEFINITIONS START]\")\n",
    "        step_def_end = response_text.find(\"[STEP DEFINITIONS END]\")\n",
    "        \n",
    "        if step_def_start >= 0 and step_def_end > step_def_start:\n",
    "            step_def_content = response_text[step_def_start + len(\"[STEP DEFINITIONS START]\"):step_def_end].strip()\n",
    "            \n",
    "            # Save step definitions file\n",
    "            step_def_file_path = os.path.join(test_output_dir, step_def_file_name)\n",
    "            with open(step_def_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(step_def_content)\n",
    "            print(f\"Created step definitions file: {step_def_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diff_source_control(code_dir):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(code_dir):\n",
    "        raise FileNotFoundError(f\"The directory {code_dir} does not exist.\")\n",
    "\n",
    "    # Get the list of changed and untracked files\n",
    "    try:\n",
    "        # Get tracked files with changes\n",
    "        changed_files_output = subprocess.check_output(\n",
    "            f\"cd {code_dir} && git diff --name-only\", shell=True, text=True\n",
    "        )\n",
    "        changed_files = changed_files_output.strip().split(\"\\n\")\n",
    "\n",
    "        # Get untracked files (excluding files in .gitignore)\n",
    "        untracked_files_output = subprocess.check_output(\n",
    "            f\"cd {code_dir} && git ls-files --others --exclude-standard\", shell=True, text=True\n",
    "        )\n",
    "        untracked_files = untracked_files_output.strip().split(\"\\n\")\n",
    "\n",
    "        # Combine tracked and untracked files\n",
    "        all_files = list(filter(None, changed_files + untracked_files))  # Remove empty strings\n",
    "\n",
    "        # Check if there are any changes or untracked files\n",
    "        if not all_files:\n",
    "            print(\"No changes or untracked files detected.\")\n",
    "        else:\n",
    "            # Create a dictionary to store file names and their diffs or content\n",
    "            file_diffs = {}\n",
    "\n",
    "            # Process each file\n",
    "            for file_name in all_files:\n",
    "                # Skip the file models/test_model.ipynb\n",
    "                if file_name.strip().startswith(\"models/\"):\n",
    "                    print(f\"Skipping file: {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if file_name in changed_files:\n",
    "                        # Get the diff for changed files\n",
    "                        diff_output = subprocess.check_output(\n",
    "                            f\"cd {code_dir} && git diff {file_name}\", shell=True, text=True\n",
    "                        )\n",
    "                        file_diffs[file_name] = f\"Diff:\\n{diff_output}\"\n",
    "                    elif file_name in untracked_files:\n",
    "                        # Get the content of untracked files\n",
    "                        file_path = os.path.join(code_dir, file_name)\n",
    "                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            file_content = f.read()\n",
    "                        file_diffs[file_name] = f\"Content:\\n{file_content}\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not process file: {file_name}. Error: {e}\")\n",
    "\n",
    "            # Save the diffs and content to a file\n",
    "            with open(diff_output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for file_name, content in file_diffs.items():\n",
    "                    f.write(f\"File: {file_name}\\n\")\n",
    "                    f.write(content)\n",
    "                    f.write(\"=\" * 80 + \"\\n\")  # Separator for readability\n",
    "\n",
    "            print(f\"Changed and untracked files have been saved to {diff_output_file_path}.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred while running git commands: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_summary(functionality_map):\n",
    "    \"\"\"\n",
    "    Generate a summary of all tests created\n",
    "    \"\"\"\n",
    "    functionalities = functionality_map.get(\"functionalities\", [])\n",
    "    \n",
    "    summary = \"# Test Suite Summary\\n\\n\"\n",
    "    summary += \"| Functionality | Test Priority | Feature File | Step Definitions |\\n\"\n",
    "    summary += \"|--------------|--------------|-------------|------------------|\\n\"\n",
    "    \n",
    "    for functionality in functionalities:\n",
    "        name = functionality[\"name\"]\n",
    "        priority = functionality.get(\"test_priority\", \"medium\")\n",
    "        sanitized_name = name.replace(\" \", \"_\").lower()\n",
    "        feature_file = f\"{sanitized_name}.feature\"\n",
    "        step_def_file = f\"{sanitized_name}_steps.java\"\n",
    "        \n",
    "        summary += f\"| {name} | {priority} | [{feature_file}](tests/{feature_file}) | [{step_def_file}](tests/{step_def_file}) |\\n\"\n",
    "    \n",
    "    # Save summary\n",
    "    with open(os.path.join(\"./output\", \"test_summary.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "    print(\"Created test summary: ./output/test_summary.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating existing tests...\n",
      "Generating code context...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Step 0: Generate code context\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating code context...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mgenerate_code_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode context saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_output_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Step 1: Update context file with existing test case context\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36mgenerate_code_context\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m controller \u001b[38;5;129;01min\u001b[39;00m controllers:\n\u001b[0;32m      4\u001b[0m     context_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTROLLER: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m controller \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     context_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../code/src/main/java/com/bank/controllers/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m context_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENTITY CONTEXT\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities:\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mget_context\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     file \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 4\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRead me this file of my java springboot application in the variable \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m and describe the functionality and context of the code written in the file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek/deepseek-r1-distill-llama-70b:free\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_utils\\_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\resources\\chat\\completions\\completions.py:914\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    913\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\openai\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m    952\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    956\u001b[0m         request,\n\u001b[0;32m    957\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    961\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:928\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    927\u001b[0m     response\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:922\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 922\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_models.py:881\u001b[0m, in \u001b[0;36mResponse.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03mRead and return the response content.\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_models.py:897\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    895\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[0;32m    898\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[0;32m    899\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_models.py:951\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[1;34m(self, chunk_size)\u001b[0m\n\u001b[0;32m    948\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[0;32m    952\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_client.py:153\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpx\\_transports\\default.py:127\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[0;32m    404\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 342\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    200\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1259\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1258\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1132\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    TASK = input(\"Enter the task to perform (GENERATE_TESTS/UPDATE_TESTS): \")\n",
    "    # TASK = \"GENERATE_TESTS\"\n",
    "    # TASK = \"UPDATE_TESTS\"\n",
    "\n",
    "    if TASK == \"GENERATE_TESTS\":\n",
    "        print(\"Starting test generation process...\")\n",
    "        \n",
    "        # Step 0: Generate code context\n",
    "        print(\"Generating code context...\")\n",
    "        generate_code_context()\n",
    "        print(f\"Code context saved to {context_output_path}\")\n",
    "\n",
    "        # Step 1: Parse context file\n",
    "        print(\"Parsing context file...\")\n",
    "        file_contexts = parse_context_file(context_file_path)\n",
    "        print(f\"Found contexts for {len(file_contexts)} files\")\n",
    "        \n",
    "        # Step 2: Create functionality map\n",
    "        print(\"Creating functionality map...\")\n",
    "        functionality_map = create_functionality_map(file_contexts)\n",
    "\n",
    "        # Save functionality map\n",
    "        with open(functionality_map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(functionality_map, f, indent=2)\n",
    "        print(f\"Functionality map saved to {functionality_map_path}\")\n",
    "\n",
    "        # Step 3: Generate test cases\n",
    "        print(\"Generating test cases...\")\n",
    "        generate_test_cases(functionality_map)\n",
    "\n",
    "        # Step 4: Generate test summary\n",
    "        print(\"Generating test summary...\")\n",
    "        generate_test_summary(functionality_map)\n",
    "\n",
    "        print(\"Test generation completed!\")\n",
    "\n",
    "    elif TASK == \"UPDATE_TESTS\":\n",
    "        print(\"Updating existing tests...\")\n",
    "\n",
    "        # Step 0: Generate code context\n",
    "        print(\"Generating code context...\")\n",
    "        generate_code_context()\n",
    "        print(f\"Code context saved to {context_output_path}\")\n",
    "        \n",
    "        # Step 1: Update context file with existing test case context\n",
    "        print(\"Updating context file with existing test case context...\")\n",
    "        update_test_context(context_file_path, test_output_dir)\n",
    "        print(\"Context file updated with test case context.\")\n",
    "\n",
    "        # Step 2: Parse context file\n",
    "        print(\"Parsing context file...\")\n",
    "        file_contexts = parse_context_file(context_file_path)\n",
    "        print(f\"Found contexts for {len(file_contexts)} files\")\n",
    "\n",
    "        # Step 3: Create functionality map\n",
    "        print(\"Creating functionality map...\")\n",
    "        functionality_test_map = create_functionality_test_map(file_contexts)\n",
    "\n",
    "        # Step 4: Find updated/changed functionalities using source control\n",
    "        generate_diff_source_control(code_dir)\n",
    "        with open(diff_output_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            diff_output = f.read()\n",
    "        print(\"Found updated/changed files using source control.\")\n",
    "\n",
    "        #Step 5: Update test cases for changed functionalities\n",
    "        print(\"Updating test cases...\")\n",
    "        update_test_cases(functionality_test_map, diff_output)\n",
    "\n",
    "        # Step 6: Generate test summary\n",
    "        print(\"Generating test summary...\")\n",
    "        generate_test_summary(functionality_test_map)\n",
    "\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
